{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [(\"The dog ate the apple\".split(),\n",
    "                  [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "                 (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\",\n",
    "                                                       \"NN\"])]\n",
    "\n",
    "word_to_idx = {}\n",
    "tag_to_idx = {}\n",
    "for context, tag in training_data:\n",
    "    for word in context:\n",
    "        if word not in word_to_idx:\n",
    "            word_to_idx[word] = len(word_to_idx)\n",
    "    for label in tag:\n",
    "        if label not in tag_to_idx:\n",
    "            tag_to_idx[label] = len(tag_to_idx)\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "character_to_idx = {}\n",
    "for i in range(len(alphabet)):\n",
    "    character_to_idx[alphabet[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharLSTM(nn.Module):\n",
    "    \"\"\"\n",
    "    对每个词的字符进行嵌入并学习得到一个结果\n",
    "    \"\"\"\n",
    "    def __init__(self, n_char, char_dim, char_hidden):\n",
    "        super(CharLSTM, self).__init__()\n",
    "        self.char_embedding = nn.Embedding(n_char, char_dim)\n",
    "        self.char_lstm = nn.LSTM(char_dim, char_hidden, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.char_embedding(x)\n",
    "        _, h = self.char_lstm(x)\n",
    "        \n",
    "        return h[0]  # 返回一个词的字符输出数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, n_word, n_char, char_dim, n_dim, char_hidden, n_hidden, n_tag):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.word_embedding = nn.Embedding(n_word, n_dim)\n",
    "        self.char_lstm = CharLSTM(n_char, char_dim, char_hidden)\n",
    "        self.lstm = nn.LSTM(n_dim + char_hidden, n_hidden, batch_first = True)\n",
    "        self.linear1 = nn.Linear(n_hidden, n_tag)\n",
    "        \n",
    "    def forward(self, x, word):\n",
    "        char = torch.FloatTensor()\n",
    "        for each in word:\n",
    "            char_list = []\n",
    "            for letter in each:\n",
    "                char_list.append(character_to_idx[letter.lower()])\n",
    "            char_list = torch.LongTensor(char_list)\n",
    "            char_list = char_list.unsqueeze(0)\n",
    "            if torch.cuda.is_available():\n",
    "                tempchar = self.char_lstm(Variable(char_list).cuda())\n",
    "            else:\n",
    "                tempchar = self.char_lstm(Variable(char_list))\n",
    "            tempchar = tempchar.squeeze(0)\n",
    "            char = torch.cat((char, tempchar.cpu().data), 0)\n",
    "        if torch.cuda.is_available():\n",
    "            char = char.cuda()\n",
    "        char = Variable(char)\n",
    "        x = self.word_embedding(x)\n",
    "        x = torch.cat((x, char), 1)\n",
    "        x = x.unsqueeze(0)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x.squeeze(0)\n",
    "        x = self.linear1(x)\n",
    "        y = F.log_softmax(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********\n",
      "epoch 1\n",
      "Loss: 1.0942506790161133\n",
      "**********\n",
      "epoch 2\n",
      "Loss: 1.0895805358886719\n",
      "**********\n",
      "epoch 3\n",
      "Loss: 1.0849435329437256\n",
      "**********\n",
      "epoch 4\n",
      "Loss: 1.0803377628326416\n",
      "**********\n",
      "epoch 5\n",
      "Loss: 1.0757620334625244\n",
      "**********\n",
      "epoch 6\n",
      "Loss: 1.0712149143218994\n",
      "**********\n",
      "epoch 7\n",
      "Loss: 1.066694736480713\n",
      "**********\n",
      "epoch 8\n",
      "Loss: 1.0622005462646484\n",
      "**********\n",
      "epoch 9\n",
      "Loss: 1.0577306747436523\n",
      "**********\n",
      "epoch 10\n",
      "Loss: 1.0532841682434082\n",
      "**********\n",
      "epoch 11\n",
      "Loss: 1.0488595962524414\n",
      "**********\n",
      "epoch 12\n",
      "Loss: 1.0444557666778564\n",
      "**********\n",
      "epoch 13\n",
      "Loss: 1.040071725845337\n",
      "**********\n",
      "epoch 14\n",
      "Loss: 1.0357060432434082\n",
      "**********\n",
      "epoch 15\n",
      "Loss: 1.0313578844070435\n",
      "**********\n",
      "epoch 16\n",
      "Loss: 1.0270261764526367\n",
      "**********\n",
      "epoch 17\n",
      "Loss: 1.0227097272872925\n",
      "**********\n",
      "epoch 18\n",
      "Loss: 1.0184077024459839\n",
      "**********\n",
      "epoch 19\n",
      "Loss: 1.014119029045105\n",
      "**********\n",
      "epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiangxinyang848/anaconda3/envs/jiang/lib/python3.5/site-packages/ipykernel_launcher.py:33: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/jiangxinyang848/anaconda3/envs/jiang/lib/python3.5/site-packages/ipykernel_launcher.py:29: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.009842872619629\n",
      "**********\n",
      "epoch 21\n",
      "Loss: 1.0055782794952393\n",
      "**********\n",
      "epoch 22\n",
      "Loss: 1.0013244152069092\n",
      "**********\n",
      "epoch 23\n",
      "Loss: 0.9970804452896118\n",
      "**********\n",
      "epoch 24\n",
      "Loss: 0.9928455948829651\n",
      "**********\n",
      "epoch 25\n",
      "Loss: 0.9886189699172974\n",
      "**********\n",
      "epoch 26\n",
      "Loss: 0.9843999147415161\n",
      "**********\n",
      "epoch 27\n",
      "Loss: 0.9801875352859497\n",
      "**********\n",
      "epoch 28\n",
      "Loss: 0.9759813547134399\n",
      "**********\n",
      "epoch 29\n",
      "Loss: 0.9717804789543152\n",
      "**********\n",
      "epoch 30\n",
      "Loss: 0.9675843119621277\n",
      "**********\n",
      "epoch 31\n",
      "Loss: 0.9633922576904297\n",
      "**********\n",
      "epoch 32\n",
      "Loss: 0.959203839302063\n",
      "**********\n",
      "epoch 33\n",
      "Loss: 0.955018162727356\n",
      "**********\n",
      "epoch 34\n",
      "Loss: 0.9508348107337952\n",
      "**********\n",
      "epoch 35\n",
      "Loss: 0.9466532468795776\n",
      "**********\n",
      "epoch 36\n",
      "Loss: 0.9424730539321899\n",
      "**********\n",
      "epoch 37\n",
      "Loss: 0.9382935762405396\n",
      "**********\n",
      "epoch 38\n",
      "Loss: 0.934114396572113\n",
      "**********\n",
      "epoch 39\n",
      "Loss: 0.9299349784851074\n",
      "**********\n",
      "epoch 40\n",
      "Loss: 0.9257550239562988\n",
      "**********\n",
      "epoch 41\n",
      "Loss: 0.921574056148529\n",
      "**********\n",
      "epoch 42\n",
      "Loss: 0.9173916578292847\n",
      "**********\n",
      "epoch 43\n",
      "Loss: 0.913207471370697\n",
      "**********\n",
      "epoch 44\n",
      "Loss: 0.9090211391448975\n",
      "**********\n",
      "epoch 45\n",
      "Loss: 0.9048323631286621\n",
      "**********\n",
      "epoch 46\n",
      "Loss: 0.900640606880188\n",
      "**********\n",
      "epoch 47\n",
      "Loss: 0.8964458703994751\n",
      "**********\n",
      "epoch 48\n",
      "Loss: 0.8922477960586548\n",
      "**********\n",
      "epoch 49\n",
      "Loss: 0.8880460262298584\n",
      "**********\n",
      "epoch 50\n",
      "Loss: 0.8838403820991516\n",
      "**********\n",
      "epoch 51\n",
      "Loss: 0.8796306252479553\n",
      "**********\n",
      "epoch 52\n",
      "Loss: 0.8754165172576904\n",
      "**********\n",
      "epoch 53\n",
      "Loss: 0.8711979389190674\n",
      "**********\n",
      "epoch 54\n",
      "Loss: 0.8669746518135071\n",
      "**********\n",
      "epoch 55\n",
      "Loss: 0.8627465963363647\n",
      "**********\n",
      "epoch 56\n",
      "Loss: 0.8585135340690613\n",
      "**********\n",
      "epoch 57\n",
      "Loss: 0.8542753458023071\n",
      "**********\n",
      "epoch 58\n",
      "Loss: 0.8500319719314575\n",
      "**********\n",
      "epoch 59\n",
      "Loss: 0.8457833528518677\n",
      "**********\n",
      "epoch 60\n",
      "Loss: 0.8415294289588928\n",
      "**********\n",
      "epoch 61\n",
      "Loss: 0.8372700810432434\n",
      "**********\n",
      "epoch 62\n",
      "Loss: 0.8330052495002747\n",
      "**********\n",
      "epoch 63\n",
      "Loss: 0.8287349939346313\n",
      "**********\n",
      "epoch 64\n",
      "Loss: 0.8244593143463135\n",
      "**********\n",
      "epoch 65\n",
      "Loss: 0.8201780319213867\n",
      "**********\n",
      "epoch 66\n",
      "Loss: 0.8158913850784302\n",
      "**********\n",
      "epoch 67\n",
      "Loss: 0.8115994334220886\n",
      "**********\n",
      "epoch 68\n",
      "Loss: 0.8073020577430725\n",
      "**********\n",
      "epoch 69\n",
      "Loss: 0.8029994368553162\n",
      "**********\n",
      "epoch 70\n",
      "Loss: 0.7986916303634644\n",
      "**********\n",
      "epoch 71\n",
      "Loss: 0.7943786978721619\n",
      "**********\n",
      "epoch 72\n",
      "Loss: 0.790060818195343\n",
      "**********\n",
      "epoch 73\n",
      "Loss: 0.7857381701469421\n",
      "**********\n",
      "epoch 74\n",
      "Loss: 0.7814106941223145\n",
      "**********\n",
      "epoch 75\n",
      "Loss: 0.7770786285400391\n",
      "**********\n",
      "epoch 76\n",
      "Loss: 0.7727423310279846\n",
      "**********\n",
      "epoch 77\n",
      "Loss: 0.7684017419815063\n",
      "**********\n",
      "epoch 78\n",
      "Loss: 0.7640571594238281\n",
      "**********\n",
      "epoch 79\n",
      "Loss: 0.7597087025642395\n",
      "**********\n",
      "epoch 80\n",
      "Loss: 0.7553566098213196\n",
      "**********\n",
      "epoch 81\n",
      "Loss: 0.7510011792182922\n",
      "**********\n",
      "epoch 82\n",
      "Loss: 0.7466425895690918\n",
      "**********\n",
      "epoch 83\n",
      "Loss: 0.7422811985015869\n",
      "**********\n",
      "epoch 84\n",
      "Loss: 0.7379170060157776\n",
      "**********\n",
      "epoch 85\n",
      "Loss: 0.7335505485534668\n",
      "**********\n",
      "epoch 86\n",
      "Loss: 0.7291820049285889\n",
      "**********\n",
      "epoch 87\n",
      "Loss: 0.7248116135597229\n",
      "**********\n",
      "epoch 88\n",
      "Loss: 0.7204396724700928\n",
      "**********\n",
      "epoch 89\n",
      "Loss: 0.7160665988922119\n",
      "**********\n",
      "epoch 90\n",
      "Loss: 0.7116926908493042\n",
      "**********\n",
      "epoch 91\n",
      "Loss: 0.7073181867599487\n",
      "**********\n",
      "epoch 92\n",
      "Loss: 0.7029433846473694\n",
      "**********\n",
      "epoch 93\n",
      "Loss: 0.6985688209533691\n",
      "**********\n",
      "epoch 94\n",
      "Loss: 0.6941946148872375\n",
      "**********\n",
      "epoch 95\n",
      "Loss: 0.6898212432861328\n",
      "**********\n",
      "epoch 96\n",
      "Loss: 0.6854490041732788\n",
      "**********\n",
      "epoch 97\n",
      "Loss: 0.6810784339904785\n",
      "**********\n",
      "epoch 98\n",
      "Loss: 0.6767096519470215\n",
      "**********\n",
      "epoch 99\n",
      "Loss: 0.6723432540893555\n",
      "**********\n",
      "epoch 100\n",
      "Loss: 0.6679794788360596\n",
      "**********\n",
      "epoch 101\n",
      "Loss: 0.6636188626289368\n",
      "**********\n",
      "epoch 102\n",
      "Loss: 0.6592617034912109\n",
      "**********\n",
      "epoch 103\n",
      "Loss: 0.6549083590507507\n",
      "**********\n",
      "epoch 104\n",
      "Loss: 0.6505592465400696\n",
      "**********\n",
      "epoch 105\n",
      "Loss: 0.6462147831916809\n",
      "**********\n",
      "epoch 106\n",
      "Loss: 0.6418754458427429\n",
      "**********\n",
      "epoch 107\n",
      "Loss: 0.637541651725769\n",
      "**********\n",
      "epoch 108\n",
      "Loss: 0.6332136392593384\n",
      "**********\n",
      "epoch 109\n",
      "Loss: 0.6288919448852539\n",
      "**********\n",
      "epoch 110\n",
      "Loss: 0.624576985836029\n",
      "**********\n",
      "epoch 111\n",
      "Loss: 0.6202691793441772\n",
      "**********\n",
      "epoch 112\n",
      "Loss: 0.6159690618515015\n",
      "**********\n",
      "epoch 113\n",
      "Loss: 0.611676812171936\n",
      "**********\n",
      "epoch 114\n",
      "Loss: 0.6073929071426392\n",
      "**********\n",
      "epoch 115\n",
      "Loss: 0.6031178832054138\n",
      "**********\n",
      "epoch 116\n",
      "Loss: 0.5988521575927734\n",
      "**********\n",
      "epoch 117\n",
      "Loss: 0.5945959687232971\n",
      "**********\n",
      "epoch 118\n",
      "Loss: 0.5903499126434326\n",
      "**********\n",
      "epoch 119\n",
      "Loss: 0.5861143469810486\n",
      "**********\n",
      "epoch 120\n",
      "Loss: 0.5818896293640137\n",
      "**********\n",
      "epoch 121\n",
      "Loss: 0.5776762962341309\n",
      "**********\n",
      "epoch 122\n",
      "Loss: 0.5734745860099792\n",
      "**********\n",
      "epoch 123\n",
      "Loss: 0.5692850351333618\n",
      "**********\n",
      "epoch 124\n",
      "Loss: 0.565108060836792\n",
      "**********\n",
      "epoch 125\n",
      "Loss: 0.5609439611434937\n",
      "**********\n",
      "epoch 126\n",
      "Loss: 0.5567931532859802\n",
      "**********\n",
      "epoch 127\n",
      "Loss: 0.5526560544967651\n",
      "**********\n",
      "epoch 128\n",
      "Loss: 0.5485330820083618\n",
      "**********\n",
      "epoch 129\n",
      "Loss: 0.5444245338439941\n",
      "**********\n",
      "epoch 130\n",
      "Loss: 0.5403308868408203\n",
      "**********\n",
      "epoch 131\n",
      "Loss: 0.536252498626709\n",
      "**********\n",
      "epoch 132\n",
      "Loss: 0.532189667224884\n",
      "**********\n",
      "epoch 133\n",
      "Loss: 0.5281427502632141\n",
      "**********\n",
      "epoch 134\n",
      "Loss: 0.5241121649742126\n",
      "**********\n",
      "epoch 135\n",
      "Loss: 0.5200982689857483\n",
      "**********\n",
      "epoch 136\n",
      "Loss: 0.5161013603210449\n",
      "**********\n",
      "epoch 137\n",
      "Loss: 0.5121217966079712\n",
      "**********\n",
      "epoch 138\n",
      "Loss: 0.508159875869751\n",
      "**********\n",
      "epoch 139\n",
      "Loss: 0.5042159557342529\n",
      "**********\n",
      "epoch 140\n",
      "Loss: 0.5002904534339905\n",
      "**********\n",
      "epoch 141\n",
      "Loss: 0.4963834285736084\n",
      "**********\n",
      "epoch 142\n",
      "Loss: 0.49249544739723206\n",
      "**********\n",
      "epoch 143\n",
      "Loss: 0.4886265993118286\n",
      "**********\n",
      "epoch 144\n",
      "Loss: 0.4847773313522339\n",
      "**********\n",
      "epoch 145\n",
      "Loss: 0.4809477925300598\n",
      "**********\n",
      "epoch 146\n",
      "Loss: 0.47713834047317505\n",
      "**********\n",
      "epoch 147\n",
      "Loss: 0.4733491539955139\n",
      "**********\n",
      "epoch 148\n",
      "Loss: 0.46958065032958984\n",
      "**********\n",
      "epoch 149\n",
      "Loss: 0.4658329486846924\n",
      "**********\n",
      "epoch 150\n",
      "Loss: 0.46210628747940063\n",
      "**********\n",
      "epoch 151\n",
      "Loss: 0.4584009349346161\n",
      "**********\n",
      "epoch 152\n",
      "Loss: 0.45471709966659546\n",
      "**********\n",
      "epoch 153\n",
      "Loss: 0.45105504989624023\n",
      "**********\n",
      "epoch 154\n",
      "Loss: 0.44741493463516235\n",
      "**********\n",
      "epoch 155\n",
      "Loss: 0.44379696249961853\n",
      "**********\n",
      "epoch 156\n",
      "Loss: 0.4402012825012207\n",
      "**********\n",
      "epoch 157\n",
      "Loss: 0.43662819266319275\n",
      "**********\n",
      "epoch 158\n",
      "Loss: 0.43307775259017944\n",
      "**********\n",
      "epoch 159\n",
      "Loss: 0.4295502007007599\n",
      "**********\n",
      "epoch 160\n",
      "Loss: 0.42604565620422363\n",
      "**********\n",
      "epoch 161\n",
      "Loss: 0.4225643277168274\n",
      "**********\n",
      "epoch 162\n",
      "Loss: 0.4191061854362488\n",
      "**********\n",
      "epoch 163\n",
      "Loss: 0.41567158699035645\n",
      "**********\n",
      "epoch 164\n",
      "Loss: 0.4122604727745056\n",
      "**********\n",
      "epoch 165\n",
      "Loss: 0.4088730812072754\n",
      "**********\n",
      "epoch 166\n",
      "Loss: 0.40550947189331055\n",
      "**********\n",
      "epoch 167\n",
      "Loss: 0.402169793844223\n",
      "**********\n",
      "epoch 168\n",
      "Loss: 0.39885398745536804\n",
      "**********\n",
      "epoch 169\n",
      "Loss: 0.3955623507499695\n",
      "**********\n",
      "epoch 170\n",
      "Loss: 0.3922947645187378\n",
      "**********\n",
      "epoch 171\n",
      "Loss: 0.3890513777732849\n",
      "**********\n",
      "epoch 172\n",
      "Loss: 0.3858322501182556\n",
      "**********\n",
      "epoch 173\n",
      "Loss: 0.38263753056526184\n",
      "**********\n",
      "epoch 174\n",
      "Loss: 0.37946707010269165\n",
      "**********\n",
      "epoch 175\n",
      "Loss: 0.376321017742157\n",
      "**********\n",
      "epoch 176\n",
      "Loss: 0.3731994032859802\n",
      "**********\n",
      "epoch 177\n",
      "Loss: 0.37010225653648376\n",
      "**********\n",
      "epoch 178\n",
      "Loss: 0.3670295476913452\n",
      "**********\n",
      "epoch 179\n",
      "Loss: 0.3639812767505646\n",
      "**********\n",
      "epoch 180\n",
      "Loss: 0.3609575629234314\n",
      "**********\n",
      "epoch 181\n",
      "Loss: 0.3579583168029785\n",
      "**********\n",
      "epoch 182\n",
      "Loss: 0.35498347878456116\n",
      "**********\n",
      "epoch 183\n",
      "Loss: 0.3520331084728241\n",
      "**********\n",
      "epoch 184\n",
      "Loss: 0.34910714626312256\n",
      "**********\n",
      "epoch 185\n",
      "Loss: 0.34620559215545654\n",
      "**********\n",
      "epoch 186\n",
      "Loss: 0.34332841634750366\n",
      "**********\n",
      "epoch 187\n",
      "Loss: 0.34047555923461914\n",
      "**********\n",
      "epoch 188\n",
      "Loss: 0.3376469612121582\n",
      "**********\n",
      "epoch 189\n",
      "Loss: 0.3348425626754761\n",
      "**********\n",
      "epoch 190\n",
      "Loss: 0.33206236362457275\n",
      "**********\n",
      "epoch 191\n",
      "Loss: 0.3293061852455139\n",
      "**********\n",
      "epoch 192\n",
      "Loss: 0.32657408714294434\n",
      "**********\n",
      "epoch 193\n",
      "Loss: 0.32386595010757446\n",
      "**********\n",
      "epoch 194\n",
      "Loss: 0.32118168473243713\n",
      "**********\n",
      "epoch 195\n",
      "Loss: 0.3185212016105652\n",
      "**********\n",
      "epoch 196\n",
      "Loss: 0.31588447093963623\n",
      "**********\n",
      "epoch 197\n",
      "Loss: 0.31327134370803833\n",
      "**********\n",
      "epoch 198\n",
      "Loss: 0.3106817305088043\n",
      "**********\n",
      "epoch 199\n",
      "Loss: 0.30811554193496704\n",
      "**********\n",
      "epoch 200\n",
      "Loss: 0.3055727183818817\n",
      "**********\n",
      "epoch 201\n",
      "Loss: 0.303053081035614\n",
      "**********\n",
      "epoch 202\n",
      "Loss: 0.3005565404891968\n",
      "**********\n",
      "epoch 203\n",
      "Loss: 0.29808294773101807\n",
      "**********\n",
      "epoch 204\n",
      "Loss: 0.2956322729587555\n",
      "**********\n",
      "epoch 205\n",
      "Loss: 0.29320433735847473\n",
      "**********\n",
      "epoch 206\n",
      "Loss: 0.290799081325531\n",
      "**********\n",
      "epoch 207\n",
      "Loss: 0.28841632604599\n",
      "**********\n",
      "epoch 208\n",
      "Loss: 0.28605586290359497\n",
      "**********\n",
      "epoch 209\n",
      "Loss: 0.28371772170066833\n",
      "**********\n",
      "epoch 210\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.28140169382095337\n",
      "**********\n",
      "epoch 211\n",
      "Loss: 0.27910757064819336\n",
      "**********\n",
      "epoch 212\n",
      "Loss: 0.2768353223800659\n",
      "**********\n",
      "epoch 213\n",
      "Loss: 0.27458474040031433\n",
      "**********\n",
      "epoch 214\n",
      "Loss: 0.27235573530197144\n",
      "**********\n",
      "epoch 215\n",
      "Loss: 0.2701480984687805\n",
      "**********\n",
      "epoch 216\n",
      "Loss: 0.267961710691452\n",
      "**********\n",
      "epoch 217\n",
      "Loss: 0.2657964527606964\n",
      "**********\n",
      "epoch 218\n",
      "Loss: 0.26365214586257935\n",
      "**********\n",
      "epoch 219\n",
      "Loss: 0.2615286707878113\n",
      "**********\n",
      "epoch 220\n",
      "Loss: 0.2594257593154907\n",
      "**********\n",
      "epoch 221\n",
      "Loss: 0.2573433816432953\n",
      "**********\n",
      "epoch 222\n",
      "Loss: 0.25528138875961304\n",
      "**********\n",
      "epoch 223\n",
      "Loss: 0.2532394826412201\n",
      "**********\n",
      "epoch 224\n",
      "Loss: 0.25121766328811646\n",
      "**********\n",
      "epoch 225\n",
      "Loss: 0.24921566247940063\n",
      "**********\n",
      "epoch 226\n",
      "Loss: 0.24723342061042786\n",
      "**********\n",
      "epoch 227\n",
      "Loss: 0.24527069926261902\n",
      "**********\n",
      "epoch 228\n",
      "Loss: 0.24332736432552338\n",
      "**********\n",
      "epoch 229\n",
      "Loss: 0.2414032220840454\n",
      "**********\n",
      "epoch 230\n",
      "Loss: 0.23949816823005676\n",
      "**********\n",
      "epoch 231\n",
      "Loss: 0.23761197924613953\n",
      "**********\n",
      "epoch 232\n",
      "Loss: 0.23574453592300415\n",
      "**********\n",
      "epoch 233\n",
      "Loss: 0.2338956594467163\n",
      "**********\n",
      "epoch 234\n",
      "Loss: 0.23206515610218048\n",
      "**********\n",
      "epoch 235\n",
      "Loss: 0.2302529513835907\n",
      "**********\n",
      "epoch 236\n",
      "Loss: 0.22845876216888428\n",
      "**********\n",
      "epoch 237\n",
      "Loss: 0.22668254375457764\n",
      "**********\n",
      "epoch 238\n",
      "Loss: 0.2249240279197693\n",
      "**********\n",
      "epoch 239\n",
      "Loss: 0.22318309545516968\n",
      "**********\n",
      "epoch 240\n",
      "Loss: 0.22145962715148926\n",
      "**********\n",
      "epoch 241\n",
      "Loss: 0.2197534143924713\n",
      "**********\n",
      "epoch 242\n",
      "Loss: 0.21806427836418152\n",
      "**********\n",
      "epoch 243\n",
      "Loss: 0.21639208495616913\n",
      "**********\n",
      "epoch 244\n",
      "Loss: 0.214736670255661\n",
      "**********\n",
      "epoch 245\n",
      "Loss: 0.21309784054756165\n",
      "**********\n",
      "epoch 246\n",
      "Loss: 0.21147549152374268\n",
      "**********\n",
      "epoch 247\n",
      "Loss: 0.209869384765625\n",
      "**********\n",
      "epoch 248\n",
      "Loss: 0.20827943086624146\n",
      "**********\n",
      "epoch 249\n",
      "Loss: 0.2067054659128189\n",
      "**********\n",
      "epoch 250\n",
      "Loss: 0.20514734089374542\n",
      "**********\n",
      "epoch 251\n",
      "Loss: 0.20360486209392548\n",
      "**********\n",
      "epoch 252\n",
      "Loss: 0.20207780599594116\n",
      "**********\n",
      "epoch 253\n",
      "Loss: 0.20056617259979248\n",
      "**********\n",
      "epoch 254\n",
      "Loss: 0.19906967878341675\n",
      "**********\n",
      "epoch 255\n",
      "Loss: 0.1975882202386856\n",
      "**********\n",
      "epoch 256\n",
      "Loss: 0.19612166285514832\n",
      "**********\n",
      "epoch 257\n",
      "Loss: 0.19466979801654816\n",
      "**********\n",
      "epoch 258\n",
      "Loss: 0.19323250651359558\n",
      "**********\n",
      "epoch 259\n",
      "Loss: 0.19180968403816223\n",
      "**********\n",
      "epoch 260\n",
      "Loss: 0.1904011070728302\n",
      "**********\n",
      "epoch 261\n",
      "Loss: 0.18900665640830994\n",
      "**********\n",
      "epoch 262\n",
      "Loss: 0.18762612342834473\n",
      "**********\n",
      "epoch 263\n",
      "Loss: 0.18625947833061218\n",
      "**********\n",
      "epoch 264\n",
      "Loss: 0.1849065124988556\n",
      "**********\n",
      "epoch 265\n",
      "Loss: 0.18356706202030182\n",
      "**********\n",
      "epoch 266\n",
      "Loss: 0.18224100768566132\n",
      "**********\n",
      "epoch 267\n",
      "Loss: 0.18092823028564453\n",
      "**********\n",
      "epoch 268\n",
      "Loss: 0.17962852120399475\n",
      "**********\n",
      "epoch 269\n",
      "Loss: 0.178341805934906\n",
      "**********\n",
      "epoch 270\n",
      "Loss: 0.17706789076328278\n",
      "**********\n",
      "epoch 271\n",
      "Loss: 0.1758066862821579\n",
      "**********\n",
      "epoch 272\n",
      "Loss: 0.17455801367759705\n",
      "**********\n",
      "epoch 273\n",
      "Loss: 0.17332175374031067\n",
      "**********\n",
      "epoch 274\n",
      "Loss: 0.1720978021621704\n",
      "**********\n",
      "epoch 275\n",
      "Loss: 0.17088600993156433\n",
      "**********\n",
      "epoch 276\n",
      "Loss: 0.1696861982345581\n",
      "**********\n",
      "epoch 277\n",
      "Loss: 0.16849827766418457\n",
      "**********\n",
      "epoch 278\n",
      "Loss: 0.16732212901115417\n",
      "**********\n",
      "epoch 279\n",
      "Loss: 0.1661575436592102\n",
      "**********\n",
      "epoch 280\n",
      "Loss: 0.16500452160835266\n",
      "**********\n",
      "epoch 281\n",
      "Loss: 0.16386282444000244\n",
      "**********\n",
      "epoch 282\n",
      "Loss: 0.16273245215415955\n",
      "**********\n",
      "epoch 283\n",
      "Loss: 0.1616131067276001\n",
      "**********\n",
      "epoch 284\n",
      "Loss: 0.1605047732591629\n",
      "**********\n",
      "epoch 285\n",
      "Loss: 0.1594073623418808\n",
      "**********\n",
      "epoch 286\n",
      "Loss: 0.15832065045833588\n",
      "**********\n",
      "epoch 287\n",
      "Loss: 0.15724459290504456\n",
      "**********\n",
      "epoch 288\n",
      "Loss: 0.1561790108680725\n",
      "**********\n",
      "epoch 289\n",
      "Loss: 0.15512391924858093\n",
      "**********\n",
      "epoch 290\n",
      "Loss: 0.15407904982566833\n",
      "**********\n",
      "epoch 291\n",
      "Loss: 0.15304434299468994\n",
      "**********\n",
      "epoch 292\n",
      "Loss: 0.15201972424983978\n",
      "**********\n",
      "epoch 293\n",
      "Loss: 0.15100501477718353\n",
      "**********\n",
      "epoch 294\n",
      "Loss: 0.15000015497207642\n",
      "**********\n",
      "epoch 295\n",
      "Loss: 0.1490049958229065\n",
      "**********\n",
      "epoch 296\n",
      "Loss: 0.148019477725029\n",
      "**********\n",
      "epoch 297\n",
      "Loss: 0.14704343676567078\n",
      "**********\n",
      "epoch 298\n",
      "Loss: 0.14607681334018707\n",
      "**********\n",
      "epoch 299\n",
      "Loss: 0.14511950314044952\n",
      "**********\n",
      "epoch 300\n",
      "Loss: 0.144171342253685\n",
      "\n",
      "tensor([[-3.8454, -0.1229, -2.3616],\n",
      "        [-3.2876, -2.4073, -0.1363],\n",
      "        [-0.1538, -2.6012, -2.6827],\n",
      "        [-2.6442, -0.1080, -3.4635]],\n",
      "       device='cuda:0', grad_fn=<LogSoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = LSTMTagger(\n",
    "    len(word_to_idx), len(character_to_idx), 10, 100, 50, 128, len(tag_to_idx))\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "\n",
    "def make_sequence(x, dic):\n",
    "    idx = [dic[i] for i in x]\n",
    "    idx = Variable(torch.LongTensor(idx))\n",
    "    return idx\n",
    "\n",
    "\n",
    "for epoch in range(300):\n",
    "    print('*' * 10)\n",
    "    print('epoch {}'.format(epoch + 1))\n",
    "    running_loss = 0\n",
    "    for data in training_data:\n",
    "        word, tag = data\n",
    "        word_list = make_sequence(word, word_to_idx)\n",
    "        tag = make_sequence(tag, tag_to_idx)\n",
    "        if torch.cuda.is_available():\n",
    "            word_list = word_list.cuda()\n",
    "            tag = tag.cuda()\n",
    "        # forward\n",
    "        out = model(word_list, word)\n",
    "        loss = criterion(out, tag)\n",
    "        running_loss += loss.data[0]\n",
    "        # backward\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Loss: {}'.format(running_loss / len(data)))\n",
    "print()\n",
    "input = make_sequence(\"Everybody ate the apple\".split(), word_to_idx)\n",
    "if torch.cuda.is_available():\n",
    "    input = input.cuda()\n",
    "\n",
    "out = model(input, \"Everybody ate the apple\".split())\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
